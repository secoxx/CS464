{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/secoxx/CS464/blob/main/CS464_Spring_2023_HW3_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmlJ7FFJGA3k"
      },
      "source": [
        "**<h1><center>CS 464</center></h1>**\n",
        "**<h1><center>Introduction to Machine Learning</center></h1>**\n",
        "**<h1><center>Spring 2023</center></h1>**\n",
        "**<h1><center>Homework 3</center></h1>**\n",
        "<h4><center>Due: June 08, 2023 23:59 (GMT+3)</center></h4>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUQigqxM4JlE"
      },
      "source": [
        "## **MNIST RGB Inpainting**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Homework Description**"
      ],
      "metadata": {
        "id": "4JLXD3sTR8Yv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, you are asked to design and train two different neural network models for image the inpainting task. In short, inpainting is a process of filling in the missing parts of an image. You will be applying this task on the MNIST RGB dataset, which is created for this homework by processing a subset of the original [MNIST](http://yann.lecun.com/exdb/mnist/) images. It contains RGB digit images from 0 to 1 colored in black, red, blue, or green. You can see a subset of the dataset below. The download link of the dataset is provided in the following parts.\n",
        "\n",
        "![MNIST RGB Samples](https://drive.google.com/uc?export=view&id=1uy0VENXeb6If-i3uxgDVtnk4qDj7eq_3)\n",
        "\n",
        "**Using PyTorch is mandatory** for this assignment. You are requested to **submit only a single *.ipynb file** in your submissions (no report needed). If you want to provide further explanations about your work, you can add Markdown cells for this purpose. From [this link](https://www.markdownguide.org/), you can get familiar with the Markdown syntax if you need. Upload your homework with the following filename convention: **\\<BilkentID\\>\\_\\<Name\\>\\_\\<Surname\\>.ipynb**\n",
        "\n",
        "Note that this assignment needs a CUDA-enabled GPU to be able to train the models in a reasonable time. If you do not have one, it is suggested to use the [Colab](https://colab.research.google.com/) environment.\n",
        "\n",
        "**Contact:** [Ahmet Burak Yıldırım](mailto:a.yildirim@bilkent.edu.tr)"
      ],
      "metadata": {
        "id": "EFNaqLRzD75v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Importing the Libraries**"
      ],
      "metadata": {
        "id": "rMvuukCeSkRE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the cell below,  some utilities that you can make use of in this assignment are imported. You can edit these imports considering your implementation as long as you use PyTorch."
      ],
      "metadata": {
        "id": "bcipuzDyJyMT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import random\n",
        "import os"
      ],
      "metadata": {
        "id": "7v0OO4-6SmNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Environment Check**"
      ],
      "metadata": {
        "id": "vLoc5OoAKDtr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the cell below, you can test whether hardware acceleration with GPU is enabled in your machine or not. If it is enabled, the printed device should be 'cuda'.\n",
        "\n",
        "**Do not change the cell**."
      ],
      "metadata": {
        "id": "KfGyOffcKG7u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Current device:', device)\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print('GPU Name:', torch.cuda.get_device_name(0))\n",
        "    print('Total GPU Memory:', round(torch.cuda.get_device_properties(0).total_memory/1024**3,1), 'GB')"
      ],
      "metadata": {
        "id": "KaTYscuOLbjc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "afc5bc48-dd69-44b2-ae1b-36181e7e4178"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current device: cuda\n",
            "GPU Name: Tesla T4\n",
            "Total GPU Memory: 14.7 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Setting Library Seeds for Reproducibility**"
      ],
      "metadata": {
        "id": "BBK2IftRSvHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To make a fair evaluation, the seed values are set for random sampling methods in PyTorch, NumPy, and Python random library.\n",
        "\n",
        "**Do not change the cell**"
      ],
      "metadata": {
        "id": "cOZICZTbMc2-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "3M5lcMwQStjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(464)"
      ],
      "metadata": {
        "id": "DJ1GUeFfSzN_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Preparing the Dataset**"
      ],
      "metadata": {
        "id": "h60dtOt5R_aE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MNIST RGB dataset is downloadable from [this link](https://drive.google.com/file/d/1xl5Ie_1c3dIg-Y0uXsgbCYEEmDfQxadZ/view?usp=share_link). If you are using Colab or a Linux machine, you can uncomment and run the below cell to download and extract the dataset automatically."
      ],
      "metadata": {
        "id": "Dzcz2jwcNKfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown # Library to download files from Google Drive\n",
        "!gdown 1xl5Ie_1c3dIg-Y0uXsgbCYEEmDfQxadZ # Google Drive ID of the zip file to be downloaded\n",
        "!unzip -oq mnist_dataset_rgb # Unzip the file downloaded. Options -o and -q overwrites the files if exists already and disables printing out the extracted files, respectively."
      ],
      "metadata": {
        "id": "85HQtprVSD3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Implementing a Custom Dataset [20 Points]**"
      ],
      "metadata": {
        "id": "ZhEiiIYxTBi9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, you are requested to implement a custom PyTorch dataset class that reads MNIST RGB images from a dataset split folder. There are two split folders called training and test in the dataset. The model class should take the root directory of a split in the \\_\\_init\\_\\_ function and read the images accordingly. Before returning the requested images, you should apply the following steps:\n",
        "\n",
        "* Apply bicubic interpolation using PIL to resize the images from (28,28) to (32,32) resolution.\n",
        "* Convert images to Tensor object\n",
        "* Normalize tensor values to scale them in the range of (-1,1)\n",
        "\n",
        "Note that reading images in the \\_\\_getitem\\_\\_ function makes the training process slow for this dataset because reading such small-sized images as a batch is slower than the forward pass process of a simple neural network. Therefore, it is suggested to read and store the images in an array in the \\_\\_init\\_\\_ function and return them in the \\_\\_getitem\\_\\_ function when they are requested by the DataLoader object."
      ],
      "metadata": {
        "id": "zGpUjWbWOBpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MnistDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.image_paths = self.get_image_paths()\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((32, 32), interpolation=Image.BICUBIC),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5,), (0.5,))\n",
        "        ])\n",
        "\n",
        "    def get_image_paths(self):\n",
        "        image_paths = []\n",
        "        for folder in os.listdir(self.root_dir):\n",
        "            folder_path = os.path.join(self.root_dir, folder)\n",
        "            image_paths.extend(\n",
        "                os.path.join(folder_path, filename)\n",
        "                for filename in os.listdir(folder_path)\n",
        "                if filename.endswith(\".jpg\")\n",
        "            )\n",
        "        return image_paths\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, data_id):\n",
        "        image_path = self.image_paths[data_id]\n",
        "        image = Image.open(image_path)\n",
        "        image = self.transform(image)\n",
        "        return image"
      ],
      "metadata": {
        "id": "ongUJiujS_UE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a dataset and a data loader object for training and test splits. Set batch sizes to 64 and 512 for training and test data loaders, respectively. Enable shuffling in the training data loader and disable it in the test data loader."
      ],
      "metadata": {
        "id": "sqaS0ePeWzd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = MnistDataset(root_dir='/content/mnist_dataset_rgb/training/')\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "test_dataset = MnistDataset(root_dir='/content/mnist_dataset_rgb/test/')\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=512, shuffle=False)"
      ],
      "metadata": {
        "id": "g-RUAYGdTuVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Do not change** the below code. If your implementation is correct, you should be seeing a grid of MNIST RGB images properly."
      ],
      "metadata": {
        "id": "XqYREQNbpQeR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Uncomment the cell when the dataloader is ready\n",
        "\n",
        "images = next(iter(train_dataloader))\n",
        "images = (images + 1) / 2\n",
        "grid_img = torchvision.utils.make_grid(images[:20], nrow=10)\n",
        "plt.axis('off')\n",
        "plt.imshow(grid_img.permute(1, 2, 0))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZnVU_VccT-Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qx_XrpE44TlW"
      },
      "source": [
        "### **Constructing Autoencoder Networks [30 Points]**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Autoencoder networks learn how to compress and reconstruct input data. It consists of two subnetworks called the encoder and the decoder. The encoder network compresses the input data, and the decoder network regenerates the data from its compressed version. In this part of the homework, you are requested to implement two different autoencoder networks, which are fully connected and convolutional autoencoders."
      ],
      "metadata": {
        "id": "EC8HgZSPmUhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Fully Connected Autoencoder [15 Points]**"
      ],
      "metadata": {
        "id": "7RAJ5jSVWQp2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fully connected networks consist of multiple [linear layers](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html). The figure of the architecture is provided below, where the number of nodes is kept small for simplification.\n",
        "\n",
        "![Fully Connected Autoencoder Architecture](https://drive.google.com/uc?export=view&id=1JeDQtjvWVoT5dZhVxesQp_XiqAnUvi1i)\n",
        "\n",
        "In this part, you are requested to implement the layers and the forward function of the model. You should flatten the input image before feeding it to the network and unflatten it when the final activations are obtained. The (input_size, output_size) pairs of the layers should be defined as follows:\n",
        "\n",
        "**Encoder:**\n",
        "- (3\\*32\\*32, 256)\n",
        "- (256, 64)\n",
        "- (64, 16)\n",
        "\n",
        "**Decoder:**\n",
        "- (16, 64)\n",
        "- (64, 256)\n",
        "- (256, 3\\*32\\*32)\n",
        "\n",
        "In each layer, [1D batch normalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) should be applied and the resulting values should be passed through a LeakyReLU layer with slope 0.2, which is the activation function. Since the image pixel value range is set to [-1,1] in the dataset, the outputs should be bounded so. Therefore, you should be using a Tanh activation function in the last layer instead of the normalization and LeakyReLU layers."
      ],
      "metadata": {
        "id": "RWwNWG7_erQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(3 * 32 * 32, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(64, 16),\n",
        "            nn.BatchNorm1d(16),\n",
        "            nn.LeakyReLU(0.2)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.Linear(16, 64),\n",
        "            nn.BatchNorm1d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(64, 256),\n",
        "            nn.BatchNorm1d(256),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(256, 3 * 32 * 32),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        decoded = decoded.view(decoded.size(0), 3, 32, 32)\n",
        "        return decoded"
      ],
      "metadata": {
        "id": "1s8WfhKnd0Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Convolutional Autoencoder [15 Points]**"
      ],
      "metadata": {
        "id": "aSBT7XjHe0h0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part, you are requested to implement an autoencoder model using convolutional layers. The architecture of a convolutional autoencoder is shown in the below figure.\n",
        "\n",
        "![Convolutional Autoencoder Architecture](https://drive.google.com/uc?export=view&id=18Ztf-zhMFC_IXDTnvGCUeGgZx-7duu0o)\n",
        "\n",
        " The (in_channel, out_channel) pairs of the layers should be defined as follows:\n",
        "\n",
        "**Encoder:**\n",
        "- (3, 16)\n",
        "- (16, 32)\n",
        "- (32, 64)\n",
        "\n",
        "**Decoder:**\n",
        "- (64, 32)\n",
        "- (32, 16)\n",
        "- (16, 3)\n",
        "\n",
        "You are free to choose the kernel and padding sizes of the layers. In each layer, [2D batch normalization](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html) should be applied and the resulting values should be passed through a LeakyReLU layer with slope 0.2, which is the activation function. Since the image pixel value range is set to [-1,1] in the dataset, the outputs should be bounded so. Therefore, you should be using a Tanh activation function in the last layer instead of the normalization and LeakyReLU layers.\n",
        "\n",
        "In the encoder part of the network, use max pooling in each layer for decreasing the resolution. The stride size should be set to one in these layers. In the decoder network, use [transposed convolution](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html) (deconvolution) layers with stride two for increasing the resolution back."
      ],
      "metadata": {
        "id": "lw4yXKnke3M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder_C(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Autoencoder_C, self).__init__()\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        )\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.ConvTranspose2d(16, 3, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded"
      ],
      "metadata": {
        "id": "LHSbmwvcUMn6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Implementing the Training Loop [15 Points]**"
      ],
      "metadata": {
        "id": "9YwxmMCrWjtb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define your training loop in this function. In the following parts, this function will be called to train both the fully connected and the convolutional autoencoders. The input arguments are provided below. Apply the training progress and return a list of losses that are calculated on each epoch. You should sum the iteration losses up during an epoch and take the mean of them to calculate the running loss of that epoch.\n",
        "\n",
        "To be able to learn inpainting, you should mask the input images as follows:\n",
        "\n",
        "![MNIST Masking](https://drive.google.com/uc?export=view&id=1gIELbRSE188PQHv_uGktPX_2cvAtMbN-)\n",
        "\n",
        "Simply, you should set the input tensor columns starting from 16 to 32 as -1 (black pixel). For the loss function, you should use the original image as the ground truth image so that the network learns how to fill the masked area of the input image and output the restored image. Before assigning the black pixels, do not forget to clone the original image to use it later in the loss function."
      ],
      "metadata": {
        "id": "6xMSf3sofmJz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "def train_autoencoder(model, dataloader, optimizer, device):\n",
        "    losses = []\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for images in dataloader:\n",
        "            images = images.to(device)\n",
        "            masked_images = images.clone()\n",
        "            masked_images[:, :, :, 16:32] = -1\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(masked_images)\n",
        "            loss = F.mse_loss(outputs, images)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        epoch_loss = running_loss / len(dataloader)\n",
        "        losses.append(epoch_loss)\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n",
        "\n",
        "    return losses"
      ],
      "metadata": {
        "id": "cifEP9HEWQde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Implementing the Evaluation Function [15 Points]**"
      ],
      "metadata": {
        "id": "ms6LRikETyds"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implement an evaluation function that returns the mean MSE calculated over the test dataset samples."
      ],
      "metadata": {
        "id": "3XTr6dpe-0NN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_autoencoder(model, dataloader, device):\n",
        "    total_loss = 0.0\n",
        "    num_samples = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images in dataloader:\n",
        "            images = images.to(device)\n",
        "            masked_images = images.clone()\n",
        "            masked_images[:, :, :, 16:32] = -1\n",
        "\n",
        "            outputs = model(masked_images)\n",
        "            loss = F.mse_loss(outputs, images, reduction='sum')\n",
        "            total_loss += loss.item()\n",
        "            num_samples += images.size(0)\n",
        "\n",
        "    mean_loss = total_loss / num_samples\n",
        "    return mean_loss"
      ],
      "metadata": {
        "id": "JW-RK7vDT3l9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Inpainting Visualization Function**"
      ],
      "metadata": {
        "id": "UjkRLMF0ZCtk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below code will be used to visualize the outputs of the trained models later.\n",
        "\n",
        "**Do not change the cell**"
      ],
      "metadata": {
        "id": "hI4ZNkVq_onm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def visualize_inpainting(model, dataset):\n",
        "    seed_everything(464)\n",
        "    dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
        "    images = next(iter(dataloader)) # Taking one batch from the dataloader\n",
        "    images = images\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "      masked_images = images.clone()\n",
        "      masked_images[:,:,:,16:] = -1\n",
        "      inpainted_images = model(masked_images.cuda()).cpu()\n",
        "    images = (images + 1) / 2\n",
        "    masked_images = (masked_images + 1) / 2\n",
        "    inpainted_images = (inpainted_images + 1) / 2\n",
        "    images_concat = torch.cat((images, masked_images, inpainted_images), dim=2)\n",
        "    grid_img = torchvision.utils.make_grid(images_concat, nrow=10)\n",
        "    plt.axis('off')\n",
        "    plt.imshow(grid_img.permute(1, 2, 0))\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VAuNmuIelqzI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training and Evaluating the FC Model [5 Points]**"
      ],
      "metadata": {
        "id": "C32DbWgOUjKZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define your loss function as MSE, set learning rate to 2e-4, create Adam optimizer, and set number of epochs to 50. Later, call the train_model function that you implemented. Visualize the returned losses on a plot (loss vs. epoch). Lastly, call evaluate_model function that you implemented and print the accuracy the model reached on the test dataset. Also, call the visualize_inpainting function to observe the final inpainting results on the test dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "lYdoryMh_9HN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "\n",
        "learning_rate = 2e-4\n",
        "num_epochs = 50\n",
        "model = Autoencoder()\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "losses = train_autoencoder(model, train_dataloader, optimizer, device)\n",
        "\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "test_loss = evaluate_autoencoder(model, test_dataloader, device)\n",
        "print(f\"Test MSE Loss: {test_loss:.4f}\")\n",
        "test_dataset = test_dataloader.dataset"
      ],
      "metadata": {
        "id": "v50u3FQAWQbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_inpainting(model, test_dataset)"
      ],
      "metadata": {
        "id": "vWbXxz6IZPBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training and Evaluating the Convolutional Model [5 Points]**"
      ],
      "metadata": {
        "id": "0JOGHIOakmL-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply the same steps given in the FC Autoencoder part."
      ],
      "metadata": {
        "id": "tvkBO1iyA7AG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(464)\n",
        "# model_conv = MnistAutoencoderConv() ## Uncomment when the model is implemented\n",
        "\n",
        "# TODO"
      ],
      "metadata": {
        "id": "8JhkdYbwiw0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(464)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "\n",
        "learning_rate = 2e-4\n",
        "num_epochs = 50\n",
        "model = Autoencoder_C()\n",
        "model = model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "\n",
        "losses = train_autoencoder(model, train_dataloader, optimizer, device)\n",
        "\n",
        "\n",
        "plt.plot(losses)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training Loss')\n",
        "plt.show()\n",
        "\n",
        "\n",
        "test_loss = evaluate_autoencoder(model, test_dataloader, device)\n",
        "print(f\"Test MSE Loss: {test_loss:.4f}\")\n",
        "test_dataset = test_dataloader.dataset\n",
        "\n",
        "visualize_inpainting(model, test_dataset)"
      ],
      "metadata": {
        "id": "IgiD1WG-ZRgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Discussion [10 Points]**"
      ],
      "metadata": {
        "id": "9yeD6vfptixK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Discuss the results that you obtained after training the models. Compare the inpainting performances by the visualizations and the accuracy metrics. You can write your answer in the below cell."
      ],
      "metadata": {
        "id": "CEgMTG1QtmtR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Inpainting results of convolutional autoencoders seems better. The reason is that, convolutional autoencoders perform better in capturing spatial information. Convolutional layers are capable of extracting local features and patterns, and in our task damaged regions generally have spatial context that needs to be taken into account. Also, convolutional layers are translation invariant, namely they can extract patterns regardless of their location in image. In our task, location of the damaged region is various on the data, so it is a desirable property and we don't know if fully connected layers have the tranlsational invariance. Lastly, convolutional autoencoders have less parameters, that is, computationally cheaper.\n",
        "---"
      ],
      "metadata": {
        "id": "ByohpqIyuU3o"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gQrmrxFzugaG"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}